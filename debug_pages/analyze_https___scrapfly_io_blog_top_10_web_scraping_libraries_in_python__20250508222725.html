<!DOCTYPE html><html class="no-js" lang="en"><head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="preconnect" href="https://cdn.scrapfly.io/" crossorigin="">
    <link rel="preconnect" href="https://apis.google.com/" crossorigin="">
    <link rel="preconnect" href="https://www.googletagmanager.com/" crossorigin="">

    <link rel="preload" href="https://cdn.scrapfly.io/0.0.155/www/public/fonts/IBMPlexMono-Regular.woff2?version=0.0.155" as="font" crossorigin="">
    <link rel="preload" href="/blog/assets/js/scrapfly.min.js?v=0.0.117_5.10.16" as="script">
    <link rel="preload" href="/blog/public/cards.min.js?v=0.0.117_5.10.16" as="script">
    <link rel="preload" href="/blog/assets/css/style.css?v=0.0.117_5.10.16" as="style">
    <link rel="preload" href="https://cdn.scrapfly.io/0.0.748/www/public/svg/logo.svg?version=0.0.748" as="image" crossorigin="">
    
    <title>Top 10 Web Scraping Packages for Python</title>
    <meta name="description" content="These are the most popular and commonly used 10 Python packages in web scraping. From HTTP connections, browser automation and data validation.">
    <link rel="icon" href="https://scrapfly.io/blog/content/images/size/w256h256/2021/12/android-chrome-512x512.png" type="image/png">
    <link rel="canonical" href="https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="ScrapFly Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Top 10 Web Scraping Packages for Python">
    <meta property="og:description" content="These are the most popular and commonly used 10 Python packages in web scraping. From HTTP connections, browser automation and data validation.">
    <meta property="og:url" content="https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
    <meta property="og:image" content="https://scrapfly.io/blog/content/images/top-10-web-scraping-libraries-in-python_banner.png">
    <meta property="article:published_time" content="2023-06-01T13:46:58.000Z">
    <meta property="article:modified_time" content="2024-08-22T12:14:27.000Z">
    <meta property="article:tag" content="Python">
    
    <meta property="article:publisher" content="https://www.facebook.com/Scrapfly">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Top 10 Web Scraping Packages for Python">
    <meta name="twitter:description" content="These are the most popular and commonly used 10 Python packages in web scraping. From HTTP connections, browser automation and data validation.">
    <meta name="twitter:url" content="https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
    <meta name="twitter:image" content="https://scrapfly.io/blog/content/images/top-10-web-scraping-libraries-in-python_banner.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Bernardas AliÅ¡auskas">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Python">
    <meta name="twitter:site" content="@Scrapfly_dev">
    <meta property="og:image:width" content="796">
    <meta property="og:image:height" content="416">
    
    <script src="https://cdn.scrapfly.io/static/fp/c.js" data-key="MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEXhnwGjm29RPoxvkqvzpXJQLAF9lt7jOvswaUDMYpJEVYRTb51+owAlZAl/8A19DtyJThVaLw5QjlUILQxeV5RQ==" async=""></script><script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "ScrapFly Blog",
        "url": "https://scrapfly.io/blog/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://scrapfly.io/blog/content/images/2021/12/logo.png"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Bernardas AliÅ¡auskas",
        "image": {
            "@type": "ImageObject",
            "url": "https://scrapfly.io/blog/content/images/2021/12/me-bw-low-contrast.jpg",
            "width": 1000,
            "height": 1000
        },
        "url": "https://scrapfly.io/blog/author/scrapecrow/",
        "sameAs": [
            "https://scrapecrow.com"
        ]
    },
    "headline": "Top 10 Web Scraping Packages for Python",
    "url": "https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/",
    "datePublished": "2023-06-01T13:46:58.000Z",
    "dateModified": "2024-08-22T12:14:27.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://scrapfly.io/blog/content/images/top-10-web-scraping-libraries-in-python_banner.svg",
        "width": 800,
        "height": 418
    },
    "keywords": "Python",
    "description": "These are the most popular and commonly used 10 Python packages in web scraping. From HTTP connections, browser automation and data validation.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://scrapfly.io/blog/"
    }
}
    </script>

    <meta name="generator" content="Ghost 5.19">
    <link rel="alternate" type="application/rss+xml" title="ScrapFly Blog" href="https://scrapfly.io/blog/rss/">
    <script async="" src="/blog/public/cards.min.js?v=0.0.117_5.10.16"></script>
    <meta name="google-site-verification" content="J5Z_OU-Ru4U4kVUiaFMY0-f1x85eHpzWs9VjdthLdWM">

    <link fetchpriority="high" rel="preload" href="https://scrapfly.io/blog/content/images/top-10-web-scraping-libraries-in-python_banner_light.svg" as="image">
    <!-- CSS Files -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/style.css?v=0.0.117_5.10.16">
<style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head>
<body class="post-template tag-python">
<header class="header sticky-top" id="home">
  <div class="container header_content">
    <nav class="navbar custom-nav navbar-expand-xl navbar-light">
      <a class="navbar-brand" href="/" id="js-animation-logo">
        <img class="logo" src="https://cdn.scrapfly.io/0.0.748/www/public/svg/logo.svg?version=0.0.748" alt="The Best Web Scraping API" title="The Best Web Scraping API" width="422" height="132">
      </a>

      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <div class="primary-nav-wrapper">
          <ul class="navbar-nav primary-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle plausible-event-name=explore plausible-event-source=nav plausible-event-interest=products" id="feature-link" href="#" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Products
              </a>
              <div class="dropdown-menu" aria-labelledby="feature-link">
                <a class="dropdown-item" href="/web-scraping-api">
                  <div class="d-flex flex-column justify-content-center">
                    <span class="fw-700 d-flex align-items-center" style="gap: 0.5em">
                      <img class="img-fluid" src="https://cdn.scrapfly.io/0.0.748/www/public/svg/home/web-scraping-api-icon.svg?version=0.0.748" alt="Web Scraping API" style="max-width:20px">
                      Web Scraping API
                    </span>
                    <small>scrape without blocks</small>
                  </div>
                </a>
                <a class="dropdown-item" href="/ai-web-scraping-api">
                  <div class="d-flex flex-column justify-content-center">
                    <span class="fw-700 d-flex align-items-center" style="gap: 0.5em">
                      <img class="img-fluid" src="https://cdn.scrapfly.io/0.0.822/www/public/svg/home/ai-web-scraping-api-icon.svg?version=0.0.822" alt="AI Web Scraping API" style="max-width:20px">
                      AI Web Scraping API
                    </span>
                    <small>hands-off web scraping</small>
                  </div>
                </a>
                <a class="dropdown-item" href="/extraction-api">
                  <div class="d-flex flex-column justify-content-center">
                    <span class="fw-700 d-flex align-items-center" style="gap: 0.5em">
                      <img class="img-fluid" src="https://cdn.scrapfly.io/0.0.748/www/public/svg/home/extraction-api-icon.svg?version=0.0.748" alt="Extraction API" style="max-width:20px">
                      Extraction API
                    </span>
                    <small>parse your documents</small>
                  </div>
                </a>
                <a class="dropdown-item" href="/screenshot-api">
                  <div class="d-flex flex-column justify-content-center">
                    <span class="fw-700 d-flex align-items-center" style="gap: 0.5em">
                      <img class="img-fluid" src="https://cdn.scrapfly.io/0.0.748/www/public/svg/home/screenshot-api-icon.svg?version=0.0.748" alt="Screenshot API" style="max-width:20px">
                      Screenshot API
                    </span>
                    <small>capture the visual web</small>
                  </div>
                </a>
              </div>
            </li>
            <li class="nav-item">
              <a class="nav-link plausible-event-name=explore plausible-event-source=nav plausible-event-interest=register" href="/pricing">Pricing</a>
            </li>
            <li class="nav-item">
              <a class="nav-link plausible-event-name=explore plausible-event-source=nav plausible-event-interest=register" href="/why-choose-scrapfly">Why Us?</a>
            </li>
            <li class="nav-item">
              <a class="nav-link plausible-event-name=explore plausible-event-source=nav plausible-event-interest=docs" data-on="click" data-event-category="navbar_docs" data-event-action="click" target="_blank" href="/docs">Docs</a>
            </li>
            <li class="nav-item">
              <a class="nav-link plausible-event-name=explore plausible-event-source=nav plausible-event-interest=content" data-on="click" data-event-category="navbar_blog" data-event-action="click" target="_blank" href="/blog/">Blog</a>
            </li>
            <li class="nav-item">
              <a class="nav-link plausible-event-name=explore plausible-event-source=nav plausible-event-interest=content" data-on="click" data-event-category="navbar_blog" data-event-action="click" target="_blank" href="/blog/knowledgebase/">Knowledgebase</a>
            </li>
          </ul>
        </div>
        <ul class="navbar-nav secondary-nav">
          <li class="nav-item">
            <a class="nav-link btn btn-primary text-light plausible-event-name=register plausible-event-source=cta" href="/register">
              Sign up
            </a>
          </li>
          <li class="nav-item nav-login-wrapper">
            <a class="nav-login nav-link" href="/login">Login</a>
          </li>
        </ul>
      </div>
    </nav>
  </div>
</header>


<div class="post-section">
    <div class="post-wrapper-content">
        <div class="content-wrapper">
            <div class="post-headlines">
                <h1>Top 10 Web Scraping Packages for Python</h1>
                <div class="post-meta">
                    <div class="post-meta__author" title="written by">
                        <a href="https://scrapfly.io/blog/author/scrapecrow/">by Bernardas AliÅ¡auskas</a>
                    </div>
                    <div class="post-meta__date">
                        <time title="originally released Jun 01, 2023">Aug 22, 2024</time>
                    </div>
                    <div class="post-meta__tags">
                        <a href="/blog/tag/python/" class="plausible-event-name=explore plausible-event-interest=content">Python</a>
                    </div>
                    <div class="post-meta__share">
                        <ul class="social-icon">
    <li>
        <a target="_blank" class="plausible-event-name=share plausible-event-share=twitter" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" href="https://twitter.com/share?text=Top%2010%20Web%20Scraping%20Packages%20for%20Python&amp;url=https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
            <svg width="25" height="20" viewBox="0 0 25 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M24.5028 2.3548C23.6012 2.7552 22.6324 3.024 21.616 3.1472C22.6548 2.5256 23.45 1.54 23.8252 0.3668C22.8536 0.9436 21.7756 1.3608 20.6332 1.5876C19.7176 0.6104 18.41 0 16.9652 0C14.1876 0 11.9364 2.2484 11.9364 5.026C11.9364 5.4208 11.9812 5.8044 12.0652 6.1712C7.8876 5.9612 4.1832 3.962 1.7052 0.9184C1.274 1.6604 1.0248 2.5256 1.0248 3.4468C1.0248 5.1912 1.9124 6.7284 3.2592 7.63C2.436 7.6048 1.6604 7.378 0.9828 7V7.0644C0.9828 9.5004 2.716 11.5304 5.0148 11.9924C4.5948 12.1072 4.1496 12.1688 3.6904 12.1688C3.3656 12.1688 3.052 12.138 2.7468 12.0792C3.3852 14.0756 5.2416 15.5288 7.4396 15.5708C5.7204 16.9176 3.556 17.7212 1.1984 17.7212C0.7924 17.7212 0.3948 17.6988 0 17.6512C2.226 19.0764 4.8664 19.908 7.7056 19.908C16.9512 19.908 22.008 12.25 22.008 5.6084C22.008 5.39 22.0024 5.1744 21.994 4.9588C22.9768 4.2476 23.8308 3.3628 24.5028 2.3548Z" fill="currentColor"></path>
</svg>        </a>
    </li>
    <li>
        <a target="_blank" class="plausible-event-name=share plausible-event-share=linkedin" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
            <svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M26.1151 1.87207H3.88505C2.82005 1.87207 1.95605 2.71507 1.95605 3.75607V26.2441C1.95605 27.2851 2.82005 28.1251 3.88505 28.1251H26.1151C27.1801 28.1251 28.0441 27.2851 28.0441 26.2441V3.75307C28.0411 2.71507 27.1771 1.87207 26.1151 1.87207ZM9.86405 23.8441H5.92505V11.9941H9.86405V23.8441V23.8441ZM7.89605 10.3771H7.86905C6.54605 10.3771 5.69405 9.46807 5.69405 8.32807C5.69405 7.16407 6.57605 6.27907 7.92005 6.27907C9.27005 6.27907 10.0981 7.16407 10.1221 8.32807C10.1221 9.46807 9.27005 10.3771 7.89605 10.3771ZM24.0721 23.8441H20.1301V17.5051C20.1301 15.9121 19.5631 14.8261 18.1381 14.8261C17.0461 14.8261 16.3981 15.5581 16.1131 16.2661C16.0111 16.5211 15.9871 16.8751 15.9871 17.2261V23.8501H12.0451C12.0451 23.8501 12.0961 13.1101 12.0451 11.9971H15.9871V13.6741C16.5091 12.8671 17.4481 11.7181 19.5361 11.7181C22.1281 11.7181 24.0721 13.4131 24.0721 17.0551V23.8441V23.8441Z" fill="currentColor"></path>
</svg>        </a>
    </li>
    <li>
        <a target="_blank" class="plausible-event-name=share plausible-event-share=facebook" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" href="https://www.facebook.com/sharer/sharer.php?u=https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
            <svg width="26" height="26" viewBox="0 0 26 26" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M18.2831 8.9908H14.6794V6.63C14.6794 5.7434 15.2696 5.5354 15.683 5.5354H18.2207V1.638L14.721 1.625C10.8392 1.625 9.95783 4.5318 9.95783 6.3934V8.9908H7.71143V13.0052H9.95783V24.3724H14.682V13.0078H17.8697L18.2831 8.9908Z" fill="currentColor"></path>
</svg>        </a>
    </li>
    <li class="copy-link-wrapper plausible-event-name=share plausible-event-share=copy">
        <a class="copy-link bg-light-red" data-link="https://scrapfly.io/blog/blog/top-10-web-scraping-libraries-in-python/" href="#">
            <svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M26.832 5.87385C25.983 4.96785 25.11 4.08585 24.204 3.23685C22.284 1.43985 19.41 1.41285 17.49 3.20385C16.119 4.48485 14.823 5.84385 13.494 7.16685C13.485 7.17585 13.482 7.18785 13.476 7.20285C13.47 7.22385 13.479 7.24785 13.497 7.25385C13.509 7.25985 13.518 7.26285 13.527 7.26285C13.575 7.26585 13.626 7.26585 13.674 7.26585C14.85 7.25085 15.957 7.50585 16.986 8.07885C17.151 8.17185 17.229 8.10285 17.331 7.99785C18.09 7.23585 18.846 6.47085 19.611 5.71785C20.334 5.00385 21.321 5.00685 22.038 5.72085C22.788 6.46485 23.535 7.21185 24.276 7.95885C24.978 8.66685 24.99 9.65685 24.285 10.3649C21.981 12.6899 19.665 15.0028 17.34 17.3098C17.142 17.5078 16.863 17.6638 16.596 17.7448C15.933 17.9488 15.375 17.7148 14.898 17.2348C14.28 16.6108 13.659 15.9929 13.05 15.3839C12.858 15.1919 12.549 15.1919 12.357 15.3839L10.737 17.0069C10.545 17.1989 10.545 17.5078 10.734 17.7028C11.394 18.3778 12.057 19.0678 12.744 19.7338C14.649 21.5818 17.703 21.5699 19.575 19.7459C20.337 19.0019 21.093 18.2519 21.843 17.4959C23.52 15.8129 25.221 14.1569 26.856 12.4349C28.551 10.6439 28.524 7.67985 26.832 5.87385ZM16.446 22.7369C16.419 22.7339 16.395 22.7338 16.368 22.7338C15.174 22.7548 14.043 22.4998 12.999 21.9178C12.828 21.8218 12.753 21.9059 12.654 22.0049C11.901 22.7579 11.151 23.5168 10.395 24.2638C9.64496 25.0048 8.67296 24.9989 7.92596 24.2549C7.22696 23.5589 6.53096 22.8599 5.83196 22.1609C4.96196 21.2909 4.95896 20.3728 5.82896 19.5028L12.504 12.8279C12.573 12.7589 12.642 12.6899 12.714 12.6239C13.242 12.1469 13.974 12.0389 14.589 12.3779C14.799 12.4919 14.985 12.6629 15.156 12.8339C15.762 13.4279 16.359 14.0309 16.95 14.6219C17.142 14.8139 17.451 14.8139 17.643 14.6219C18.192 14.0729 18.738 13.5299 19.302 12.9689C19.494 12.7769 19.491 12.4709 19.299 12.2789C18.525 11.5079 17.766 10.7129 16.935 10.0019C15.123 8.44485 12.3 8.51385 10.554 10.1459C9.31196 11.3099 8.12996 12.5309 6.92696 13.7339C5.66396 14.9999 4.37696 16.2418 3.15596 17.5498C1.43096 19.3948 1.45496 22.3018 3.18296 24.1528C4.02296 25.0528 4.89596 25.9229 5.79296 26.7659C7.69196 28.5509 10.578 28.5869 12.483 26.8109C13.86 25.5239 15.165 24.1589 16.503 22.8299C16.512 22.8209 16.515 22.8058 16.518 22.7878C16.518 22.7638 16.488 22.7399 16.446 22.7369V22.7369Z" fill="currentColor"></path>
</svg>
            <span class="copied">Copied to clipboard</span>
        </a>
    </li>
</ul>                    </div>
                </div>
            </div>
        </div>

        <div class="content-wrapper">
            <div class="post-full-content">
                <!--kg-card-begin: html-->
        <figure class="kg-card kg-image-card">
          <img fetchpriority="high" loading="eager" src="https://scrapfly.io/blog/content/images/top-10-web-scraping-libraries-in-python_banner_light.svg" class="kg-image medium-zoom-image" alt="Top 10 Web Scraping Packages for Python" width="800" height="418">
        </figure>
        <!--kg-card-end: html--><!--kg-card-begin: markdown--><p>Python is by far the most popular language used for web scraping. It's easy to learn, has a huge community and a massive ecosystem of libraries.</p>
<p>In this quick overview article, we'll be taking a look at the top 10 web scraping packages that every web scraper should know. Covering various niches like:</p>
<ul>
<li>HTTP Connections</li>
<li>Browser Automation</li>
<li>HTML and JSON Parsing</li>
<li>Data Quality and Validation</li>
</ul>
<p><em>we use all of these libraries in our <a href="https://scrapfly.io/blog/tag/scrapeguide/" target="_blank" rel="noopener noreferrer" class="shortcode-url">web scraping guide series</a> if you want to see them in action</em></p>
<aside class="toc" style="top: 100px;"><div class="toc-wrapper"><ol class="toc-list "><li class="toc-list-item"><a href="#httpx" class="toc-link node-name--H2 ">HTTPX</a></li><li class="toc-list-item"><a href="#parsel-and-lxml" class="toc-link node-name--H2 ">Parsel and LXML</a></li><li class="toc-list-item"><a href="#beautifulsoup" class="toc-link node-name--H2 ">BeautifulSoup</a></li><li class="toc-list-item"><a href="#jmespath-and-jsonpath" class="toc-link node-name--H2 ">JMESPath and JSONPath</a></li><li class="toc-list-item"><a href="#playwright-and-selenium" class="toc-link node-name--H2 ">Playwright and Selenium</a></li><li class="toc-list-item"><a href="#cerberus-and-pydantic" class="toc-link node-name--H2 ">Cerberus and Pydantic</a></li><li class="toc-list-item"><a href="#scrapfly-python-sdk" class="toc-link node-name--H2 ">Scrapfly Python SDK</a></li><li class="toc-list-item is-active-li"><a href="#summary" class="toc-link node-name--H2  is-active-link">Summary</a></li></ol></div><div class="toc-extras"><div class="toc-cta newsletter">
          <button class="toc-cta-btn plausible-event-name=newsletter plausible-event-source=toc-cta btn btn-primary btn-lg btn-block newsletter">
            JOIN THE NEWSLETTER
          </button>
          <p class="toc-cta-note">
            Get monthly web scraping insights ðŸ‘†
          </p>
          <a href="https://scrapfly.io/academy" target="_blank" class="toc-cta-link plausible-event-name=explore plausible-event-interest=academy plausible-event-source=toc-cta">
            Learn at ScrapFly Academy
          </a>
      </div></div></aside>
<h2 id="httpx">HTTPX</h2>
<p><a href="https://pypi.org/project/httpx/" target="_blank" rel="noopener noreferrer" class="shortcode-url">HTTPX</a> is by far the most complete and modern HTTP client package for Python. It is inspired by the popular <a href="https://pypi.org/project/requests/" target="_blank" rel="noopener noreferrer" class="shortcode-url">requests</a> library but it takes things even further by supporting modern Python and HTTP features.</p>
<p>To start, it supports both asynchronous and synchronous Python APIs. This makes it easy to scale httpx-powered requests while also staying accessible in simple scripts:</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> httpx

<span class="hljs-comment"># use Sync API</span>
<span class="hljs-keyword">with</span> httpx.Client() <span class="hljs-keyword">as</span> client:
    response = client.get(<span class="hljs-string">"https://web-scraping.dev/product/1"</span>)
    <span class="hljs-built_in">print</span>(response.text)

<span class="hljs-comment"># use Async API:</span>
<span class="hljs-keyword">import</span> asyncio

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> httpx.AsyncClient() <span class="hljs-keyword">as</span> client:
        response = <span class="hljs-keyword">await</span> client.get(<span class="hljs-string">"https://web-scraping.dev/product/1"</span>)
        <span class="hljs-built_in">print</span>(response.text)

asyncio.run(run())
</code></pre>
<p>HTTPX also supports HTTP/2 which is much less likely to be blocked as most real human traffic is using http2/3:</p>
<pre><code class="language-python"><span class="hljs-keyword">with</span> httpx.Client(http2=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> client:
    response = client.get(<span class="hljs-string">"https://web-scraping.dev/product/1"</span>)
    <span class="hljs-built_in">print</span>(response.text)
</code></pre>
<p>Finally, <code>httpx</code> respects HTTP standards and forms requests more similarly to a real web browser which can drastically reduce the likelyhood of being blocked. Like <a class="text-reference shortcode-tref" href="https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-headers/#request-header-order">respecting header ordering</a>:</p>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/web-scraping-with-python-httpx/#using-httpx">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">How to Web Scrape with HTTPX and Python</div>
<div class="kg-bookmark-description">
<p>See our full introduction to HTTPX use in web scraping and solutions to commonly encountered issues like retries and blocking.</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/web-scraping-with-python-httpx_banner_light.svg" alt="How to Web Scrape with HTTPX and Python" width="800" height="418">
</div>
</a>
</figure>
<h2 id="parsel-and-lxml">Parsel and LXML</h2>
<p><a href="https://pypi.org/project/lxml/" target="_blank" rel="noopener noreferrer" class="shortcode-url">LXML</a> is a fast and feature-rich HTML/XML parser for Python. It is a wrapper around the C library <code>libxml2</code> and it is the fastest and most reliable way to parse HTML in Python.</p>
<p>LXML uses powerful and flexible <a class="text-reference shortcode-tref" href="https://scrapfly.io/blog/parsing-html-with-xpath/">XPath Selectors</a> to find elements in HTML:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> html
html = <span class="hljs-string">"""
&lt;html&gt;
  &lt;title&gt;My Title&lt;/title&gt;
  &lt;body&gt;
    &lt;div class="title"&gt;Product Title&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
"""</span>
tree = html.fromstring(html)
<span class="hljs-built_in">print</span>(tree.xpath(<span class="hljs-string">"//title/text()"</span>))
[<span class="hljs-string">'My Title'</span>]
<span class="hljs-built_in">print</span>(tree.xpath(<span class="hljs-string">"//div[@class='title']/text()"</span>))
[<span class="hljs-string">'Product Title'</span>]
</code></pre>
<p>To take things even further, <code>lxml</code> is extended by <a href="https://pypi.org/project/parsel/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Parsel</a> which simplifies the use API and adds <a class="text-reference shortcode-tref" href="https://scrapfly.io/blog/parsing-html-with-css/">CSS selector</a> support:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> parsel <span class="hljs-keyword">import</span> Selector

html = <span class="hljs-string">"""
&lt;html&gt;
  &lt;title&gt;My Title&lt;/title&gt;
  &lt;body&gt;
    &lt;div class="title"&gt;Product Title&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
"""</span>

selector = Selector(html)
<span class="hljs-comment"># use XPath</span>
<span class="hljs-built_in">print</span>(selector.xpath(<span class="hljs-string">"//title/text()"</span>).get())
<span class="hljs-string">"My Title"</span>
<span class="hljs-comment"># or CSS selectors</span>
<span class="hljs-built_in">print</span>(selector.css(<span class="hljs-string">".title::text"</span>).get())
<span class="hljs-string">"Product Title"</span>
</code></pre>
<p>Parsel is becoming the de facto way to parse large sets of HTML documents as it combines the speed of <code>lxml</code> with the ease of use of CSS and XPath selectors.</p>
<h2 id="beautifulsoup">BeautifulSoup</h2>
<p><a href="https://pypi.org/project/beautifulsoup4/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Beautifulsoup</a> (aka bs4) is another HTML parser library in Python. Though it's much more than that.</p>
<p>Unlike LXML and Parsel, bs4 support parsing using accessible Pythonic methods like <code>find</code> and <code>find_all</code>:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup

html = <span class="hljs-string">"""
&lt;html&gt;
  &lt;title&gt;My Title&lt;/title&gt;
  &lt;body&gt;
    &lt;div class="title"&gt;Product Title&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
"""</span>

soup = BeautifulSoup(html, <span class="hljs-string">'lxml'</span>)  <span class="hljs-comment"># note: bs4 can use lxml under the hood which makes it super fast!</span>
<span class="hljs-comment"># find single element by node name</span>
<span class="hljs-built_in">print</span>(soup.find(<span class="hljs-string">"title"</span>).text)
<span class="hljs-string">"My Title"</span>
<span class="hljs-comment"># find multiple using find_all and attribute matching</span>
<span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> soup.find_all(<span class="hljs-string">"div"</span>, class_=<span class="hljs-string">"title"</span>):
    <span class="hljs-built_in">print</span>(element.text)
</code></pre>
<p>This approach is more accessible and more readable than CSS or XPath selectors for beginners and often easier to maintain and develop when working with highly complex HTML documents.</p>
<p>BeautilfulSoup4 also comes with many utility functions like HTML formatting and editing. For example:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup

html = <span class="hljs-string">"""
&lt;div&gt;&lt;h1&gt;The Avangers: &lt;/h1&gt;&lt;a&gt;End Game&lt;/a&gt;&lt;p&gt;is one of the most popular Marvel movies&lt;/p&gt;&lt;/div&gt;
"""</span>
soup = BeautifulSoup(html)
soup.prettify()
<span class="hljs-string">"""
&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;h1&gt;
    The Avangers:
   &lt;/h1&gt;
   &lt;a&gt;
    End Game
   &lt;/a&gt;
   &lt;p&gt;
    is one of the most popular Marvel movies
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;
"""</span>
</code></pre>
<p>As well as many other utility functions like HTML modification, selective parsing and cleanup.</p>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/web-scraping-with-python-beautifulsoup/#parsing-html-with-beautifulsoup">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">Web Scraping with Python and BeautifulSoup</div>
<div class="kg-bookmark-description">
<p>See our complete introduction to web scraping with beautifulsoup, all of it's utilities and an example project</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/web-scraping-with-python-beautifulsoup_banner_light.svg" alt="Web Scraping with Python and BeautifulSoup" width="1910" height="1000">
</div>
</a>
</figure>
<h2 id="jmespath-and-jsonpath">JMESPath and JSONPath</h2>
<p><a href="https://pypi.org/project/jmespath/" target="_blank" rel="noopener noreferrer" class="shortcode-url">JMESPath</a> and <a href="https://pypi.org/project/jsonpath-ng/" target="_blank" rel="noopener noreferrer" class="shortcode-url">JSONPath</a> are two libraries that allow you to query JSON data using a query language similar to XPath.</p>
<p>As JSON is becoming bigger and bigger in the web scraping space, these libraries are becoming essential.</p>
<p>For example, with JMESPath we can query and reshape and flatten JSON datasets with ease:</p>
<pre><code class="language-python">data = {
  <span class="hljs-string">"people"</span>: [
    {
      <span class="hljs-string">"name"</span>: <span class="hljs-string">"Foo"</span>, 
      <span class="hljs-string">"age"</span>: <span class="hljs-number">33</span>, 
      <span class="hljs-string">"addresses"</span>: [
        <span class="hljs-string">"123 Main St"</span>, <span class="hljs-string">"California"</span>, <span class="hljs-string">"US"</span>
      ],
      <span class="hljs-string">"primary_email"</span>: <span class="hljs-string">"foo@email.com"</span>,
      <span class="hljs-string">"secondary_email"</span>: <span class="hljs-string">"bar@email.com"</span>,
    },
    ...
  ]
}
jmespath.search(<span class="hljs-string">"""
  people[].{
    first_name: name, 
    age_in_years: age,
    address: addresses[0],
    state: addresses[1],
    country: addresses[2],
    emails: [primary_email, secondary_email]
  }
"""</span>, data)
[
  {
    <span class="hljs-string">'address'</span>: <span class="hljs-string">'123 Main St'</span>,
    <span class="hljs-string">'state'</span>: <span class="hljs-string">'California'</span>
    <span class="hljs-string">'country'</span>: <span class="hljs-string">'US'</span>,
    <span class="hljs-string">'age_in_years'</span>: <span class="hljs-number">33</span>,
    <span class="hljs-string">'emails'</span>: [<span class="hljs-string">'foo@email.com'</span>, <span class="hljs-string">'bar@email.com'</span>],
    <span class="hljs-string">'first_name'</span>: <span class="hljs-string">'Foo'</span>,
  },
  ...
]
</code></pre>
<p>This feature is especially useful when scraping <a class="text-reference shortcode-tref" href="https://scrapfly.io/blog/how-to-scrape-hidden-web-data/">hidden web data</a> which can result in <em>massive</em> JSON datasets that are hard to navigate and digest.</p>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/parse-json-jmespath-python/#using-jmespath">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">Quick Intro to Parsing JSON with JMESPath in Python</div>
<div class="kg-bookmark-description">
<p>See our full introduction to JMESPath and an example web scraping project with it.</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/parse-json-jmespath-python_banner_light.svg" alt="Quick Intro to Parsing JSON with JMESPath in Python" width="3024" height="1580">
</div>
</a>
</figure>
<p>Alternatively, JSONPath focuses less on reshaping datasets and more on selecting values from complex, heavily nested JSON datasets and supports advanced matchign functions:</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> jsonpath_ng.ext <span class="hljs-keyword">as</span> jp

data = {
    <span class="hljs-string">"products"</span>: [
        {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Apple"</span>, <span class="hljs-string">"price"</span>: <span class="hljs-number">12.88</span>, <span class="hljs-string">"tags"</span>: [<span class="hljs-string">"fruit"</span>, <span class="hljs-string">"red"</span>]},
        {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Peach"</span>, <span class="hljs-string">"price"</span>: <span class="hljs-number">27.25</span>, <span class="hljs-string">"tags"</span>: [<span class="hljs-string">"fruit"</span>, <span class="hljs-string">"yellow"</span>]},
        {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Cake"</span>, <span class="hljs-string">"tags"</span>: [<span class="hljs-string">"pastry"</span>, <span class="hljs-string">"sweet"</span>]},
    ]
}

<span class="hljs-comment"># find all product names:</span>
query = jp.parse(<span class="hljs-string">"products[*].name"</span>)
<span class="hljs-keyword">for</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">in</span> query.find(data):
    <span class="hljs-built_in">print</span>(<span class="hljs-keyword">match</span>.value)

<span class="hljs-comment"># find all products with price &gt; 20</span>
query = jp.parse(<span class="hljs-string">"products[?price&gt;20].name"</span>)
<span class="hljs-keyword">for</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">in</span> query.find(data):
    <span class="hljs-built_in">print</span>(<span class="hljs-keyword">match</span>.value)
</code></pre>
<p>The killer feature of JSONPath is the recursive <code>$..</code> selector which allows to quickly select values by key anywhere in the dataset (similar to the beloved XPath's <code>//</code> selector):</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> jsonpath_ng.ext <span class="hljs-keyword">as</span> jp

data = {
    <span class="hljs-string">"products"</span>: [
        {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Apple"</span>, <span class="hljs-string">"price"</span>: <span class="hljs-number">12.88</span>},
        {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Peach"</span>, <span class="hljs-string">"price"</span>: <span class="hljs-number">27.25</span>},
        {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Cake"</span>},
        {<span class="hljs-string">"multiproduct"</span>: [{<span class="hljs-string">"name"</span>: <span class="hljs-string">"Carrot"</span>}, {<span class="hljs-string">"name"</span>: <span class="hljs-string">"Pineapple"</span>}]}
    ]
}

<span class="hljs-comment"># find all "name" fields no matter where they are:</span>
query = jp.parse(<span class="hljs-string">"$..name"</span>)
<span class="hljs-keyword">for</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">in</span> query.find(data):
    <span class="hljs-built_in">print</span>(<span class="hljs-keyword">match</span>.value)
</code></pre>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/parse-json-jsonpath-python/#introduction-to-jsonpath">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">Quick Intro to Parsing JSON with JSONPath in Python</div>
<div class="kg-bookmark-description">
<p>See our full introduction to JSONPath and common challenges it helps to solve through an example project.</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/parse-json-jsonpath-python_banner_light.svg" alt="Quick Intro to Parsing JSON with JSONPath in Python" width="3024" height="1580">
</div>
</a>
</figure>
<h2 id="playwright-and-selenium">Playwright and Selenium</h2>
<p><a class="text-reference shortcode-tref" href="https://scrapfly.io/blog/scraping-using-browsers/">Headless browsers</a> are becoming very popular in web scraping as a way to deal with dynamic javascript and scraper blocking.</p>
<p>Many websites use complex front-end that generated data on demand either through background requests or javascript functions. To access this data we need a javascript execution environment and there's no better way than to employ a real headless web browser.</p>
<p><a class="text-reference shortcode-tref" href="https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-javascript/">Javascript fingerprinting</a> is also becoming a required step to scrape many modern websites.</p>
<p>So, to control a headless browser for web scraping in Python we have two popular libraries: <a href="https://pypi.org/project/playwright/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Playwright</a> and <a href="https://pypi.org/project/selenium/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Selenium</a>.</p>
<p><a href="https://pypi.org/project/selenium/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Selenium</a> is one of the first major browser automation libraries and it can do almost anything a browser can do:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver
<span class="hljs-keyword">from</span> selenium.webdriver.common.by <span class="hljs-keyword">import</span> By
<span class="hljs-keyword">from</span> selenium.webdriver.chrome.options <span class="hljs-keyword">import</span> Options
<span class="hljs-keyword">from</span> selenium.webdriver.support.ui <span class="hljs-keyword">import</span> WebDriverWait
<span class="hljs-keyword">from</span> selenium.webdriver.support <span class="hljs-keyword">import</span> expected_conditions <span class="hljs-keyword">as</span> EC
<span class="hljs-keyword">import</span> time

<span class="hljs-comment"># configure browser</span>
options = Options()
options.headless = <span class="hljs-literal">True</span> 
options.add_argument(<span class="hljs-string">"--window-size=1920,1080"</span>)
options.add_argument(<span class="hljs-string">"start-maximized"</span>)

driver = webdriver.Chrome(options=options)
driver.get(<span class="hljs-string">"https://web-scraping.dev/product/1"</span>)
<span class="hljs-comment"># wait for page to load by waiting for reviews to appear on the page</span>
element = WebDriverWait(driver=driver, timeout=<span class="hljs-number">5</span>).until(
    EC.presence_of_element_located((By.CSS_SELECTOR, <span class="hljs-string">'#reviews'</span>))
)
<span class="hljs-comment"># click buttons:</span>
button = wait.until(EC.element_to_be_clickable((By.ID, <span class="hljs-string">"load-more-reviews"</span>)))
button.click()

<span class="hljs-comment"># return rendered HTML:</span>
<span class="hljs-built_in">print</span>(driver.page_source)
</code></pre>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/web-scraping-with-selenium-and-python/#navigating-waiting-and-retrieving">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">Web Scraping with Selenium and Python Tutorial + Example Project</div>
<div class="kg-bookmark-description">
<p>For more see our full introduction to web scraping with Selenium through an example Python project</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/web-scraping-with-selenium-and-python_banner_light.svg" alt="Web Scraping with Selenium and Python Tutorial + Example Project" width="1910" height="1000">
</div>
</a>
</figure>
<p>Alternatively, <a href="https://pypi.org/project/playwright/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Playwright</a> is a modern take on browser automation offering all of these capabilities in modern asynchronous and synchronous APIs:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> playwright.sync_api <span class="hljs-keyword">import</span> sync_playwright

<span class="hljs-comment"># Using synchronous Python:</span>
<span class="hljs-keyword">with</span> sync_playwright() <span class="hljs-keyword">as</span> p:
    browser = p.chromium.launch(headless=<span class="hljs-literal">True</span>)
    context = browser.new_context(viewport={<span class="hljs-string">'width'</span>: <span class="hljs-number">1920</span>, <span class="hljs-string">'height'</span>: <span class="hljs-number">1080</span>})
    page = context.new_page()
    page.goto(<span class="hljs-string">"https://web-scraping.dev/product/1"</span>)

    <span class="hljs-comment"># wait for the element to be present in the DOM</span>
    page.wait_for_selector(<span class="hljs-string">'#reviews'</span>)

    <span class="hljs-comment"># click the button</span>
    page.wait_for_selector(<span class="hljs-string">"#load-more-reviews"</span>, state=<span class="hljs-string">"attached"</span>)
    page.click(<span class="hljs-string">"#load-more-reviews"</span>)

    <span class="hljs-comment"># print the HTML</span>
    <span class="hljs-built_in">print</span>(page.content())

    browser.close()

<span class="hljs-comment"># or asynchronous</span>
<span class="hljs-keyword">import</span> asyncio
<span class="hljs-keyword">from</span> playwright.async_api <span class="hljs-keyword">import</span> async_playwright

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">with</span> async_playwright() <span class="hljs-keyword">as</span> p:
        browser = <span class="hljs-keyword">await</span> p.chromium.launch(headless=<span class="hljs-literal">True</span>)
        context = <span class="hljs-keyword">await</span> browser.new_context(viewport={<span class="hljs-string">'width'</span>: <span class="hljs-number">1920</span>, <span class="hljs-string">'height'</span>: <span class="hljs-number">1080</span>})
        page = <span class="hljs-keyword">await</span> context.new_page()
        <span class="hljs-keyword">await</span> page.goto(<span class="hljs-string">"https://web-scraping.dev/product/1"</span>)

        <span class="hljs-comment"># wait for the element to be present in the DOM</span>
        <span class="hljs-keyword">await</span> page.wait_for_selector(<span class="hljs-string">'#reviews'</span>)

        <span class="hljs-comment"># click the button</span>
        <span class="hljs-keyword">await</span> page.wait_for_selector(<span class="hljs-string">"#load-more-reviews"</span>, state=<span class="hljs-string">"attached"</span>)
        <span class="hljs-keyword">await</span> page.click(<span class="hljs-string">"#load-more-reviews"</span>)

        <span class="hljs-comment"># print the HTML</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-keyword">await</span> page.content())

        <span class="hljs-keyword">await</span> browser.close()

asyncio.run(run())
</code></pre>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/web-scraping-with-playwright-and-python/#the-basics">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">Web Scraping with Playwright and Python</div>
<div class="kg-bookmark-description">
<p>See our full introduction to web scraping using Playwright through an example project for more.</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/web-scraping-with-playwright-and-python_banner_light.svg" alt="Web Scraping with Playwright and Python" width="1910" height="1000">
</div>
</a>
</figure>
<h2 id="cerberus-and-pydantic">Cerberus and Pydantic</h2>
<p>An often overlooked process of web scraping is the data quality assurance step. Web scraping is a unique niche where result datasets are highly dynamic making quality testing very difficult.</p>
<p>Fortunately, there are several tools that can help out with ensuring web scraped data quality.</p>
<p>For web scraping applications that require real-time data validation <a href="https://pypi.org/project/pydantic/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Pydantic</a> is a great choice. Pydantic allows specifying strict data models that can be used to validate and morph scraped data:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span>
<span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> BaseModel, validator

<span class="hljs-comment"># example for scraped company data</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Company</span>(<span class="hljs-title class_ inherited__">BaseModel</span>):
    <span class="hljs-comment"># define allowed field names and types:</span>
    size: <span class="hljs-built_in">int</span>
    founded: <span class="hljs-built_in">int</span>
    revenue_currency: <span class="hljs-built_in">str</span>
    hq_city: <span class="hljs-built_in">str</span>
    hq_state: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>]  <span class="hljs-comment"># some fields can be optional (i.e. have value of None)</span>

    <span class="hljs-comment"># then we can define any custom validation functions:</span>
<span class="hljs-meta">    @validator(<span class="hljs-params"><span class="hljs-string">"size"</span></span>)</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">must_be_reasonable_size</span>(<span class="hljs-params">cls, v</span>):
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (<span class="hljs-number">0</span> &lt; v &lt; <span class="hljs-number">20_000</span>):
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f"unreasonable company size: <span class="hljs-subst">{v}</span>"</span>)
        <span class="hljs-keyword">return</span> v

<span class="hljs-meta">    @validator(<span class="hljs-params"><span class="hljs-string">"founded"</span></span>)</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">must_be_reasonable_year</span>(<span class="hljs-params">cls, v</span>):
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (<span class="hljs-number">1900</span> &lt; v &lt; <span class="hljs-number">2022</span>):
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f"unreasonable found date: <span class="hljs-subst">{v}</span>"</span>)
        <span class="hljs-keyword">return</span> v

<span class="hljs-meta">    @validator(<span class="hljs-params"><span class="hljs-string">"hq_state"</span></span>)</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">looks_like_state</span>(<span class="hljs-params">cls, v</span>):
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(v) != <span class="hljs-number">2</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f'state should be 2 character long, got "<span class="hljs-subst">{v}</span>"'</span>)
        <span class="hljs-keyword">return</span> v
</code></pre>
<p>For data web scrapers that require more flexibility and less strict data validation <a href="https://pypi.org/project/Cerberus/" target="_blank" rel="noopener noreferrer" class="shortcode-url">Cerberus</a> is a great choice. Cerberus is a schema validation library that allows specifying data models using a simple dictionary syntax:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> cerberus <span class="hljs-keyword">import</span> Validator


<span class="hljs-keyword">def</span> <span class="hljs-title function_">validate_name</span>(<span class="hljs-params">field, value, error</span>):
    <span class="hljs-string">"""function for validating"""</span>
    <span class="hljs-keyword">if</span> <span class="hljs-string">"."</span> <span class="hljs-keyword">in</span> value:
        error(field, <span class="hljs-string">f"contains a dot character: <span class="hljs-subst">{value}</span>"</span>)
    <span class="hljs-keyword">if</span> value.lower() <span class="hljs-keyword">in</span> [<span class="hljs-string">"classified"</span>, <span class="hljs-string">"redacted"</span>, <span class="hljs-string">"missing"</span>]:
        error(field, <span class="hljs-string">f"redacted value: <span class="hljs-subst">{value}</span>"</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-string">"&lt;"</span> <span class="hljs-keyword">in</span> value.lower() <span class="hljs-keyword">and</span> <span class="hljs-string">"&gt;"</span> <span class="hljs-keyword">in</span> value.lower():
        error(field, <span class="hljs-string">f"contains html nodes: <span class="hljs-subst">{value}</span>"</span>)


schema = {
    <span class="hljs-string">"name"</span>: {
        <span class="hljs-comment"># name should be a string</span>
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"string"</span>,
        <span class="hljs-comment"># between 2 and 20 characters</span>
        <span class="hljs-string">"minlength"</span>: <span class="hljs-number">2</span>,
        <span class="hljs-string">"maxlength"</span>: <span class="hljs-number">20</span>,
        <span class="hljs-comment"># extra validation</span>
        <span class="hljs-string">"check_with"</span>: validate_name,
    },
}

v = Validator(schema)

v.validate({<span class="hljs-string">"name"</span>: <span class="hljs-string">"H."</span>})
<span class="hljs-built_in">print</span>(v.errors)
<span class="hljs-comment"># {'name': ['contains a dot character: H.']}</span>
</code></pre>
<p>The killer feature of Cerberus is the ease of use and ability to define flexible schemas that can work with highly dynamic web-scraped data. At Scrapfly, we use Cerberus to test all of <a href="https://github.com/scrapfly/scrapfly-scrapers">our example scrapers</a>.</p>
<figure class="kg-card kg-bookmark-card kg-card-hascaption shortcode-ref">
<a class="kg-bookmark-container" href="https://scrapfly.io/blog/how-to-ensure-web-scrapped-data-quality/#cerberus-or-pydantic">
<div class="kg-bookmark-content">
<div class="kg-bookmark-title">How to Ensure Web Scrapped Data Quality</div>
<div class="kg-bookmark-description">
<p>See our complete introduction to data validation which covers both Pydantic and Cerberus use in web scraping</p>
</div>
</div>
<div class="kg-bookmark-thumbnail">
<img src="https://scrapfly.io/blog/content/images/how-to-ensure-web-scrapped-data-quality_banner_light.svg" alt="How to Ensure Web Scrapped Data Quality" width="3024" height="1580">
</div>
</a>
</figure>
<h2 id="scrapfly-python-sdk">Scrapfly Python SDK</h2>
<div class="scrapfly-pitch shortcode-pitch">
<p>
ScrapFly provides <a href="https://scrapfly.io/docs/scrape-api/getting-started" target="_blank">web scraping</a>, <a href="https://scrapfly.io/docs/screenshot-api/getting-started" target="_blank">screenshot</a>, and <a href="https://scrapfly.io/docs/extraction-api/getting-started" target="_blank">extraction</a> APIs for data collection at scale.
</p>
<ul>
<li><a href="https://scrapfly.io/docs/scrape-api/anti-scraping-protection" target="_blank">Anti-bot protection bypass</a> - scrape web pages without blocking!</li>    
<li><a href="https://scrapfly.io/docs/scrape-api/proxy" target="_blank">Rotating residential proxies</a> - prevent IP address and geographic blocks.</li>    
<li><a href="https://scrapfly.io/docs/scrape-api/javascript-rendering" target="_blank">JavaScript rendering</a> - scrape dynamic web pages through cloud browsers.</li>
<li><a href="https://scrapfly.io/docs/scrape-api/javascript-scenario" target="_blank">Full browser automation</a> - control browsers to scroll, input and click on objects.</li>
<li><a href="https://scrapfly.io/docs/scrape-api/getting-started#api_param_format" target="_blank">Format conversion</a> - scrape as HTML, JSON, Text, or Markdown.</li>
<li><a href="https://scrapfly.io/docs/sdk/python" target="_blank">Python</a> and 
<a href="https://scrapfly.io/docs/sdk/typescript" target="_blank">Typescript</a> SDKs, as well as 
<a href="https://scrapfly.io/docs/sdk/scrapy" target="_blank">Scrapy</a> and 
<a href="https://scrapfly.io/docs/integration/getting-started" target="_blank">no-code tool integrations</a>.
</li>
</ul>
</div>
<figure class="kg-card kg-image-card kg-card-hascaption shortcode-img">
<img class="kg-image medium-zoom-image" loading="lazy" src="https://scrapfly.io/blog/content/images/common_scrapfly-api.svg" alt="scrapfly middleware" title="" width="300" height="146">
</figure>
<p>For example, HTML can be parsed directly using <code>.selector</code> attribute of a Scrapfly result:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> scrapfly <span class="hljs-keyword">import</span> ScrapeConfig, ScrapflyClient
scrapfly = ScrapflyClient(key=<span class="hljs-string">"YOUR KEY"</span>)
result = scrapfly.scrape(ScrapeConfig(<span class="hljs-string">"https://httpbin.dev/html"</span>))

<span class="hljs-comment"># scrapfly result build parsel.Selector automatically:</span>
page_title = result.selector.xpath(<span class="hljs-string">"//h1/text()"</span>).get()
<span class="hljs-string">"Herman Melville - Moby-Dick"</span>
</code></pre>
<h2 id="summary">Summary</h2>
<p>In this article, we've covered the top 10 Python packages for web scraping that cover several steps of the web scraping process.</p>
<p>For web scraping connections, we've covered <code>httpx</code> which is a brilliant, feature-rich HTTP client. On the other hand, there's <code>selenium</code> and <code>playwright</code> which use a whole headless web browser.</p>
<p>For data parsing, we've taken a look at the three most popular HTML parsing tools: <code>lxml</code>, <code>parsel</code> and <code>beautifulsoup</code>. For JSON parsing <code>jmespath</code> and <code>jsonpath</code> which work great in web scraping environments.</p>
<p>Finally, for the final step of a web scraping process, we've looked at two different data validation tools: strict <code>pydantic</code> and the flexible <code>cerberus</code>.</p>
<!--kg-card-end: markdown-->
            </div>
            <div class="ctas container"><div class="row">
<div class="col-md-6">
    <a class="cta light plausible-event-name=explore plausible-event-interest=docs" href="https://scrapfly.io/docs/sdk/python">Check out ScrapFly Python SDK</a>
</div>
<div class="col-md-6"><a class="cta plausible-event-name=register plausible-event-source=cta" href="https://scrapfly.io/register">Try ScrapFly for FREE!</a></div>
</div>
</div>            <div class="post-content-footer">
                <div id="related-questions">
                        <div class="related-page-wrapper">
        <h5>Related Questions</h5>
        <div><ul>
            <li><a class="related-page" href="/blog/what-python-libraries-support-http2/"><span class="icon"></span>What Python libraries support HTTP2?</a></li>
            <li><a class="related-page" href="/blog/httpx-vs-requests-vs-aiohttp/"><span class="icon"></span>Python httpx vs requests vs aiohttp - key differences</a></li>
            <li><a class="related-page" href="/blog/html-table-to-xlsx-python-beautifulsoup/"><span class="icon"></span>How to scrape HTML table to Excel Spreadsheet (.xlsx)?</a></li>
            <li><a class="related-page" href="/blog/how-to-click-on-alert-dialog-in-playwright/"><span class="icon"></span>How to handle popup dialogs in Playwright?</a></li>
            <li><a class="related-page" href="/blog/how-to-use-proxies-python-httpx/"><span class="icon"></span>How to use proxies with Python httpx?</a></li>
            <li><a class="related-page" href="/blog/how-to-scrape-images-from-website/"><span class="icon"></span>How to scrape images from a website?</a></li>
            <li><a class="related-page" href="/blog/how-to-select-dictionary-key-recursively-in-python/"><span class="icon"></span>How to select dictionary key recursively in Python?</a></li>
            <li><a class="related-page" href="/blog/how-to-check-for-element-in-playwright/"><span class="icon"></span>How to check if element exists in Playwright?</a></li>
            <li><a class="related-page" href="/blog/what-are-some-ways-to-parse-json-datasets-in-python/"><span class="icon"></span>What are some ways to parse JSON datasets in Python?</a></li>
            <li><a class="related-page" href="/blog/how-to-use-curl-in-python/"><span class="icon"></span>How to use cURL in Python?</a></li>
            <li><a class="related-page" href="/blog/selenium-chromedriver-in-path/"><span class="icon"></span>Selenium: chromedriver executable needs to be in PATH?</a></li>
            <li><a class="related-page" href="/blog/selenium-geckodriver-in-path/"><span class="icon"></span>Selenium: geckodriver executable needs to be in PATH?</a></li>
            <li><a class="related-page" href="/blog/python-requests-exception-connecttimeout/"><span class="icon"></span>How to fix python requests ConnectTimeout error?</a></li>
            <li><a class="related-page" href="/blog/python-requests-exception-readtimeout/"><span class="icon"></span>How to fix Python requests ReadTimeout error?</a></li>
            <li><a class="related-page" href="/blog/python-requests-exception-missingschema/"><span class="icon"></span>How to fix Python requests MissingSchema error?</a></li>
        </ul></div>
        <div class="mt-2 text-right"><a href="/blog/knowledgebase/">More &gt;</a></div>
    </div>    
                </div>
                <div class="d-flex gap-2">
                    <ul class="sb-tag-list">
                        <li><a href="/blog/tag/python/">Python</a></li>
                    </ul>
                    <ul class="social-icon">
    <li>
        <a target="_blank" class="plausible-event-name=share plausible-event-share=twitter" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" href="https://twitter.com/share?text=Top%2010%20Web%20Scraping%20Packages%20for%20Python&amp;url=https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
            <svg width="25" height="20" viewBox="0 0 25 20" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M24.5028 2.3548C23.6012 2.7552 22.6324 3.024 21.616 3.1472C22.6548 2.5256 23.45 1.54 23.8252 0.3668C22.8536 0.9436 21.7756 1.3608 20.6332 1.5876C19.7176 0.6104 18.41 0 16.9652 0C14.1876 0 11.9364 2.2484 11.9364 5.026C11.9364 5.4208 11.9812 5.8044 12.0652 6.1712C7.8876 5.9612 4.1832 3.962 1.7052 0.9184C1.274 1.6604 1.0248 2.5256 1.0248 3.4468C1.0248 5.1912 1.9124 6.7284 3.2592 7.63C2.436 7.6048 1.6604 7.378 0.9828 7V7.0644C0.9828 9.5004 2.716 11.5304 5.0148 11.9924C4.5948 12.1072 4.1496 12.1688 3.6904 12.1688C3.3656 12.1688 3.052 12.138 2.7468 12.0792C3.3852 14.0756 5.2416 15.5288 7.4396 15.5708C5.7204 16.9176 3.556 17.7212 1.1984 17.7212C0.7924 17.7212 0.3948 17.6988 0 17.6512C2.226 19.0764 4.8664 19.908 7.7056 19.908C16.9512 19.908 22.008 12.25 22.008 5.6084C22.008 5.39 22.0024 5.1744 21.994 4.9588C22.9768 4.2476 23.8308 3.3628 24.5028 2.3548Z" fill="currentColor"></path>
</svg>        </a>
    </li>
    <li>
        <a target="_blank" class="plausible-event-name=share plausible-event-share=linkedin" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
            <svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M26.1151 1.87207H3.88505C2.82005 1.87207 1.95605 2.71507 1.95605 3.75607V26.2441C1.95605 27.2851 2.82005 28.1251 3.88505 28.1251H26.1151C27.1801 28.1251 28.0441 27.2851 28.0441 26.2441V3.75307C28.0411 2.71507 27.1771 1.87207 26.1151 1.87207ZM9.86405 23.8441H5.92505V11.9941H9.86405V23.8441V23.8441ZM7.89605 10.3771H7.86905C6.54605 10.3771 5.69405 9.46807 5.69405 8.32807C5.69405 7.16407 6.57605 6.27907 7.92005 6.27907C9.27005 6.27907 10.0981 7.16407 10.1221 8.32807C10.1221 9.46807 9.27005 10.3771 7.89605 10.3771ZM24.0721 23.8441H20.1301V17.5051C20.1301 15.9121 19.5631 14.8261 18.1381 14.8261C17.0461 14.8261 16.3981 15.5581 16.1131 16.2661C16.0111 16.5211 15.9871 16.8751 15.9871 17.2261V23.8501H12.0451C12.0451 23.8501 12.0961 13.1101 12.0451 11.9971H15.9871V13.6741C16.5091 12.8671 17.4481 11.7181 19.5361 11.7181C22.1281 11.7181 24.0721 13.4131 24.0721 17.0551V23.8441V23.8441Z" fill="currentColor"></path>
</svg>        </a>
    </li>
    <li>
        <a target="_blank" class="plausible-event-name=share plausible-event-share=facebook" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" href="https://www.facebook.com/sharer/sharer.php?u=https://scrapfly.io/blog/top-10-web-scraping-libraries-in-python/">
            <svg width="26" height="26" viewBox="0 0 26 26" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M18.2831 8.9908H14.6794V6.63C14.6794 5.7434 15.2696 5.5354 15.683 5.5354H18.2207V1.638L14.721 1.625C10.8392 1.625 9.95783 4.5318 9.95783 6.3934V8.9908H7.71143V13.0052H9.95783V24.3724H14.682V13.0078H17.8697L18.2831 8.9908Z" fill="currentColor"></path>
</svg>        </a>
    </li>
    <li class="copy-link-wrapper plausible-event-name=share plausible-event-share=copy">
        <a class="copy-link bg-light-red" data-link="https://scrapfly.io/blog/blog/top-10-web-scraping-libraries-in-python/" href="#">
            <svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M26.832 5.87385C25.983 4.96785 25.11 4.08585 24.204 3.23685C22.284 1.43985 19.41 1.41285 17.49 3.20385C16.119 4.48485 14.823 5.84385 13.494 7.16685C13.485 7.17585 13.482 7.18785 13.476 7.20285C13.47 7.22385 13.479 7.24785 13.497 7.25385C13.509 7.25985 13.518 7.26285 13.527 7.26285C13.575 7.26585 13.626 7.26585 13.674 7.26585C14.85 7.25085 15.957 7.50585 16.986 8.07885C17.151 8.17185 17.229 8.10285 17.331 7.99785C18.09 7.23585 18.846 6.47085 19.611 5.71785C20.334 5.00385 21.321 5.00685 22.038 5.72085C22.788 6.46485 23.535 7.21185 24.276 7.95885C24.978 8.66685 24.99 9.65685 24.285 10.3649C21.981 12.6899 19.665 15.0028 17.34 17.3098C17.142 17.5078 16.863 17.6638 16.596 17.7448C15.933 17.9488 15.375 17.7148 14.898 17.2348C14.28 16.6108 13.659 15.9929 13.05 15.3839C12.858 15.1919 12.549 15.1919 12.357 15.3839L10.737 17.0069C10.545 17.1989 10.545 17.5078 10.734 17.7028C11.394 18.3778 12.057 19.0678 12.744 19.7338C14.649 21.5818 17.703 21.5699 19.575 19.7459C20.337 19.0019 21.093 18.2519 21.843 17.4959C23.52 15.8129 25.221 14.1569 26.856 12.4349C28.551 10.6439 28.524 7.67985 26.832 5.87385ZM16.446 22.7369C16.419 22.7339 16.395 22.7338 16.368 22.7338C15.174 22.7548 14.043 22.4998 12.999 21.9178C12.828 21.8218 12.753 21.9059 12.654 22.0049C11.901 22.7579 11.151 23.5168 10.395 24.2638C9.64496 25.0048 8.67296 24.9989 7.92596 24.2549C7.22696 23.5589 6.53096 22.8599 5.83196 22.1609C4.96196 21.2909 4.95896 20.3728 5.82896 19.5028L12.504 12.8279C12.573 12.7589 12.642 12.6899 12.714 12.6239C13.242 12.1469 13.974 12.0389 14.589 12.3779C14.799 12.4919 14.985 12.6629 15.156 12.8339C15.762 13.4279 16.359 14.0309 16.95 14.6219C17.142 14.8139 17.451 14.8139 17.643 14.6219C18.192 14.0729 18.738 13.5299 19.302 12.9689C19.494 12.7769 19.491 12.4709 19.299 12.2789C18.525 11.5079 17.766 10.7129 16.935 10.0019C15.123 8.44485 12.3 8.51385 10.554 10.1459C9.31196 11.3099 8.12996 12.5309 6.92696 13.7339C5.66396 14.9999 4.37696 16.2418 3.15596 17.5498C1.43096 19.3948 1.45496 22.3018 3.18296 24.1528C4.02296 25.0528 4.89596 25.9229 5.79296 26.7659C7.69196 28.5509 10.578 28.5869 12.483 26.8109C13.86 25.5239 15.165 24.1589 16.503 22.8299C16.512 22.8209 16.515 22.8058 16.518 22.7878C16.518 22.7638 16.488 22.7399 16.446 22.7369V22.7369Z" fill="currentColor"></path>
</svg>
            <span class="copied">Copied to clipboard</span>
        </a>
    </li>
</ul>                </div>
            </div>
        </div>
    </div>
    <div class="container">
            <div class="related-post-wrapper">
        <!--related post item -->
        <div class="row">
            <div class="col-md-8 mx-auto text-center">
                <div class="related-post-heading">
                    <h2>Related Posts</h2>
                </div>
            </div>
        </div>
        <div class="row posts justify-content-center">
                <div class="post-card js-post-card">
    <div class="single-post-card post tag-crawling tag-beautifulsoup tag-python">
        <div class="feature">
            <div class="feature_image">
                <a href="/blog/guide-to-list-crawling/">
                    <img width="1910" height="1000" src="/blog/content/images/guide-to-list-crawling_banner.svg" alt="Guide to List Crawling: Everything You Need to Know">
                </a>
            </div>
        </div>
        <div class="post-content">
            <a class="post-card-link" href="/blog/guide-to-list-crawling/"></a>
            <div class="post-content-inner">
                <time class="card-meta-date">
                    Mar 10, 2025
                </time>
                <h5 class="post-title">
                    <a href="/blog/guide-to-list-crawling/">Guide to List Crawling: Everything You Need to Know</a>
                </h5>
                <p class="excerpt-text">In-depth look at list crawling - how to extract valuable data from list-formatted content like tables, listicles and paginated pages.</p>
            </div>

            <ul class="card-tag-list">
                <li><a href="/blog/tag/crawling/">Web Crawling</a></li>
                <li><a href="/blog/tag/beautifulsoup/">Beautifulsoup</a></li>
                <li><a href="/blog/tag/python/">Python</a></li>
            </ul>
        </div>
    </div>
</div>                <div class="post-card js-post-card">
    <div class="single-post-card post tag-crawling tag-python">
        <div class="feature">
            <div class="feature_image">
                <a href="/blog/how-to-find-all-urls-on-a-domain/">
                    <img width="800" height="418" src="/blog/content/images/how-to-find-all-urls-on-a-domain_banner.svg" alt="How to Find All URLs on a Domain">
                </a>
            </div>
        </div>
        <div class="post-content">
            <a class="post-card-link" href="/blog/how-to-find-all-urls-on-a-domain/"></a>
            <div class="post-content-inner">
                <time class="card-meta-date">
                    Jan 29, 2025
                </time>
                <h5 class="post-title">
                    <a href="/blog/how-to-find-all-urls-on-a-domain/">How to Find All URLs on a Domain</a>
                </h5>
                <p class="excerpt-text">Learn how to efficiently find all URLs on a domain using Python and web crawling. Guide on how to crawl entire domain to collect all website data</p>
            </div>

            <ul class="card-tag-list">
                <li><a href="/blog/tag/crawling/">Web Crawling</a></li>
                <li><a href="/blog/tag/python/">Python</a></li>
            </ul>
        </div>
    </div>
</div>                <div class="post-card js-post-card">
    <div class="single-post-card post tag-screenshots tag-python tag-nodejs">
        <div class="feature">
            <div class="feature_image">
                <a href="/blog/screenshot-to-pdf/">
                    <img width="1600" height="836" src="/blog/content/images/screenshot-to-pdf_banner.svg" alt="How to Capture and Convert a Screenshot to PDF">
                </a>
            </div>
        </div>
        <div class="post-content">
            <a class="post-card-link" href="/blog/screenshot-to-pdf/"></a>
            <div class="post-content-inner">
                <time class="card-meta-date">
                    Jan 22, 2025
                </time>
                <h5 class="post-title">
                    <a href="/blog/screenshot-to-pdf/">How to Capture and Convert a Screenshot to PDF</a>
                </h5>
                <p class="excerpt-text">Quick guide on how to effectively capture web screenshots as PDF documents</p>
            </div>

            <ul class="card-tag-list">
                <li><a href="/blog/tag/screenshots/">Screenshots</a></li>
                <li><a href="/blog/tag/python/">Python</a></li>
                <li><a href="/blog/tag/nodejs/">NodeJS</a></li>
            </ul>
        </div>
    </div>
</div>        </div>
    </div>    
    </div>
</div>



<!--- START FOOTER SECTION -->
<div class="footer-section">
  <footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-2">
                <h3>Company</h3>
                <ul class="list-unstyled">
                    <li><a href="/jobs" data-on="click" data-event-category="footer-careers" data-event-action="click">Careers</a></li>
                    <li><a href="/terms-of-service" data-on="click" data-event-category="footer-tos" data-event-action="click">Terms of service</a></li>
                    <li><a href="/privacy-policy" data-on="click" data-event-category="footer-privacy-policy" data-event-action="click">Privacy Policy</a></li>
                    <li><a href="/data-processing-agreement" data-on="click" data-event-category="footer-dpa" data-event-action="click">Data Processing Agreement</a></li>
                    <li><a href="/kyc-and-safety" data-on="click" data-event-category="footer-kyc" data-event-action="click">KYC Compliance</a></li>
                    <li><a href="https://scrapfly.statuspage.io/" target="_blank" rel="noopener" data-on="click" data-event-category="footer-status" data-event-action="click">Status</a></li>
                </ul>
                <a class="&quot;plausible-event-name=explore" plausible-event-source="footer" plausible-event-interest="integration&quot;" href="/integration"><h3>Integrations</h3></a>
                <ul class="list-unstyled">
                    <li><a class="&quot;plausible-event-name=explore" plausible-event-source="footer" plausible-event-interest="integration&quot;" href="/integration/zapier">Zapier</a></li>
                    <li><a class="&quot;plausible-event-name=explore" plausible-event-source="footer" plausible-event-interest="integration&quot;" href="/integration/make">Make</a></li>
                    <li><a class="&quot;plausible-event-name=explore" plausible-event-source="footer" plausible-event-interest="integration&quot;" href="/integration/n8n">N8n</a></li>
                    <li><a class="&quot;plausible-event-name=explore" plausible-event-source="footer" plausible-event-interest="integration&quot;" href="/integration/llamaindex">LlamaIndex</a></li>
                    <li><a class="&quot;plausible-event-name=explore" plausible-event-source="footer" plausible-event-interest="integration&quot;" href="/integration/langchain">LangChain</a></li>
                </ul>
                <h3>Social</h3>
                <div class="social d-flex align-content-center">
                    <a href="https://github.com/scrapfly" rel="noopener" target="_blank" aria-label="Github">
                      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
    <path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z" fill="currentColor"></path>
</svg>                    </a>
                    <a href="https://www.linkedin.com/company/scrapfly" rel="noopener" target="_blank" aria-label="LinkedIn">
                      <svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M26.1151 1.87207H3.88505C2.82005 1.87207 1.95605 2.71507 1.95605 3.75607V26.2441C1.95605 27.2851 2.82005 28.1251 3.88505 28.1251H26.1151C27.1801 28.1251 28.0441 27.2851 28.0441 26.2441V3.75307C28.0411 2.71507 27.1771 1.87207 26.1151 1.87207ZM9.86405 23.8441H5.92505V11.9941H9.86405V23.8441V23.8441ZM7.89605 10.3771H7.86905C6.54605 10.3771 5.69405 9.46807 5.69405 8.32807C5.69405 7.16407 6.57605 6.27907 7.92005 6.27907C9.27005 6.27907 10.0981 7.16407 10.1221 8.32807C10.1221 9.46807 9.27005 10.3771 7.89605 10.3771ZM24.0721 23.8441H20.1301V17.5051C20.1301 15.9121 19.5631 14.8261 18.1381 14.8261C17.0461 14.8261 16.3981 15.5581 16.1131 16.2661C16.0111 16.5211 15.9871 16.8751 15.9871 17.2261V23.8501H12.0451C12.0451 23.8501 12.0961 13.1101 12.0451 11.9971H15.9871V13.6741C16.5091 12.8671 17.4481 11.7181 19.5361 11.7181C22.1281 11.7181 24.0721 13.4131 24.0721 17.0551V23.8441V23.8441Z" fill="currentColor"></path>
</svg>                    </a>
                    <a href="https://www.youtube.com/@scrapfly" rel="noopener" target="_blank" aria-label="YouTube">
                      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512">
<path d="M549.7 124.1c-6.3-23.7-24.8-42.3-48.3-48.6C458.8 64 288 64 288 64S117.2 64 74.6 75.5c-23.5 6.3-42 24.9-48.3 48.6-11.4 42.9-11.4 132.3-11.4 132.3s0 89.4 11.4 132.3c6.3 23.7 24.8 41.5 48.3 47.8C117.2 448 288 448 288 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zm-317.5 213.5V175.2l142.7 81.2-142.7 81.2z" fill="currentColor"></path>
</svg>                    </a>
                </div>
            </div>
            <div class="col-md-2">
                <h3>Tools</h3>
                <ul class="list-unstyled">
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=tools" href="/web-scraping-tools/curl-python">Convert cURL commands to Python code</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=tools" href="/web-scraping-tools/ja3-fingerprint">JA3/TLS Fingerprint</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=tools" href="/web-scraping-tools/http2-fingerprint">HTTP2 Fingerprint</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=tools" href="/web-scraping-tools/css-xpath-tester">Xpath/CSS Selector Tester</a></li>
                </ul>
                <h3>Resources</h3>
                                            <ul class="list-unstyled">
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=docs" href="/web-scraping-tools/ja3-fingerprint" data-on="click" data-event-category="footer-docs" data-event-action="click">API Documentation</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/academy" data-on="click" data-event-category="academy" data-event-action="click">Web Scraping Academy</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/is-web-scraping-legal" data-on="click" data-event-category="footer-scrape-legal" data-event-action="click">Is Web Scraping Legal?</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=tools" href="/web-scraping-tools" data-on="click" data-event-category="footer-tools" data-event-action="click">Web Scraping Tools</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/faq" data-on="click" data-event-category="footer-faq" data-event-action="click">FAQ</a></li>
                </ul>
            </div>
            <div class="col-md-4">
                <h3>Learn Web Scraping</h3>
                                            <ul class="list-unstyled">
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/everything-to-know-about-web-scraping-python/">Web Scraping with Python</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-php-101/">Web Scraping with PHP</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-ruby/">Web Scraping with Ruby</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-r/">Web Scraping with R</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-nodejs/">Web Scraping with NodeJS</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-scrapy/">Web Scraping with Python Scrapy</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/how-to-scrape-without-getting-blocked-tutorial/">How to Scrape without getting blocked tutorial</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-python-beautifulsoup/">Web Scraping with Python and BeautifulSoup</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-with-puppeteer-and-nodejs/">Web Scraping with Nodejs and Puppeteer</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/web-scraping-graphql-with-python/">How To Scrape Graphql</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/best-proxy-providers-for-web-scraping/">Best Proxies for Web Scraping</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="https://scrapfly.io/blog/top-5-residential-proxy-providers/">Top 5 Best Residential Proxies</a></li>
                </ul>
            </div>
            <div class="col-md-4">
                <h3>Usage</h3>
                                            <ul class="list-unstyled">
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/web-scraping">What is Web Scraping used for?</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/ai-training-web-scraping">Web Scraping for AI Training</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/compliance-web-scraping">Web Scraping for Compliance</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/ecommerce-web-scraping">Web Scraping for eCommerce</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/finance-web-scraping">Web Scraping for Finance</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/fraud-detection-web-scraping">Web Scraping for Fraud Detection</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/jobs-web-scraping">Web Scraping for Jobs</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/web-scraping-leads">Web Scraping for Lead Generation</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/media-and-news-web-scraping">Web Scraping for News &amp; Media</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/real-estate-web-scraping">Web Scraping for Real Estate</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/seo-and-serp-web-scraping">Web Scraping for SERP &amp; SEO</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/social-media-web-scraping">Web Scraping for Social Media</a></li>
                    <li><a class="plausible-event-name=explore plausible-event-source=footer plausible-event-interest=content" href="/use-case/travel-web-scraping">Web Scraping for Travel</a></li>
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-12 mb-3 text-center">
                Â© 2025 Scrapfly - The Best Web Scraping API For Developers
            </div>
        </div>
    </div>
</footer>
</div><script>
    var pagination_next_page_number = '',
        pagination_available_pages_number = '';
</script>

<script src="/blog/assets/js/scrapfly.min.js?v=0.0.117_5.10.16"></script>


<div id="icons" class="d-none">
    <div id="icon-academy">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"><g style="fill:#fff"><path d="M1087-226a.733.733 0 0 0-.72.597.646.646 0 0 0-.107-.01h-.02c-.01 0-.012 0-.019.002a.697.697 0 0 0-.648.814.698.698 0 0 0-.594.689c0 .042.01.085.014.13a.921.921 0 0 0-.075 1.364 1.077 1.077 0 0 0 .197 1.59.826.826 0 0 0 .251.851.83.83 0 0 0 .711.19.93.93 0 0 0 1.742-.454v-5.03a.736.736 0 0 0-.734-.735zm.506 5.76a.701.701 0 0 1-1.347.274.114.114 0 0 0-.141-.064.593.593 0 0 1-.587-.118.602.602 0 0 1-.16-.683v-.004a.622.622 0 0 1 .58-.378c.153 0 .153-.23 0-.23-.294 0-.57.15-.727.4a.85.85 0 0 1-.1-1.246.926.926 0 0 0 1.391-.798c0-.156-.231-.156-.228 0a.697.697 0 0 1-1.392 0 .701.701 0 0 1 .306-.554.114.114 0 0 0 .045-.13.453.453 0 0 1-.025-.14c0-.258.209-.468.472-.468l.024.003a.115.115 0 0 0 .118-.145v-.007c0-.014 0-.027-.01-.04a.448.448 0 0 1-.028-.15c0-.255.205-.463.46-.467a.469.469 0 0 1 .458.468c0 .153.23.153.23 0a.698.698 0 0 0-.353-.605.504.504 0 0 1 1.006.055v5.03z" style="fill:#fff;stroke-width:.0763" transform="matrix(3.5471 0 0 3.5471 -3847.064 801.682)"></path><path d="M94 45.3c0-3.68-1.66-7.12-4.49-9.41.12-.561.182-1.13.184-1.71 0-4.58-3.39-8.38-7.78-9.04.099-.513.149-1.03.149-1.56 0-4.88-3.84-8.86-8.66-9.11a1.49 1.49 0 0 0-.242-.024c-.04 0-.079.005-.12.006-.04-.001-.079-.006-.12-.006-.458 0-.919.041-1.41.126a9.586 9.586 0 0 0-9.44-7.82c-5.31 0-9.63 4.32-9.63 9.63v65.9c0 6.72 5.47 12.2 12.2 12.2 4.46 0 8.51-2.41 10.6-6.25a9.43 9.43 0 0 0 1.4.198c2.9.185 5.73-.766 7.93-2.69 2.19-1.91 3.52-4.61 3.7-7.51.077-1.23-.06-2.46-.403-3.64 3.8-2.69 6.03-6.95 6.03-11.6 0-3.38-1.22-6.64-3.44-9.22 2.24-2.27 3.51-5.3 3.51-8.46zM90.94 63c0 3.48-1.58 6.68-4.3 8.82-2.01-3.2-5.57-5.24-9.43-5.24-2 0-2 3 0 3a8.157 8.157 0 0 1 7.58 5.21c.41 1.09.592 2.19.521 3.27a7.922 7.922 0 0 1-2.68 5.45 7.865 7.865 0 0 1-5.75 1.95c-.594-.039-1.21-.167-1.93-.402a1.507 1.507 0 0 0-1.85.84 9.164 9.164 0 0 1-8.46 5.6c-5.07 0-9.19-4.12-9.19-9.19v-65.9a6.635 6.635 0 0 1 6.63-6.63c3.4 0 6.19 2.56 6.56 5.91-2.75 1.58-4.61 4.54-4.61 7.93 0 2 3 2 3 0 0-3.34 2.69-6.06 6.02-6.13 3.33.065 6.02 2.79 6.02 6.13 0 .622-.117 1.27-.359 1.97-.07.202-.094.417-.074.63l-.01.035a1.505 1.505 0 0 0 1.55 1.87c.131-.01.262-.026.392-.046a6.15 6.15 0 0 1 6.14 6.14c0 .572-.103 1.16-.322 1.85-.203.635.038 1.33.591 1.7a9.1 9.1 0 0 1 4.01 7.24l-.001.012c0 5.03-4.09 9.12-9.12 9.12-5.03 0-9.12-4.09-9.12-9.12 0-2-3-2-3 0 0 6.68 5.44 12.1 12.1 12.1 2.23 0 4.31-.615 6.11-1.67 1.92 2.09 2.98 4.76 2.98 7.55z" style="fill:#fff" transform="matrix(.27072 0 0 .27072 -1.447 -1.662)"></path></g></svg>
    </div>
</div>

<div id="newsletterModal" class="modal" style="display: none;">
  <div class="modal-content">
    <span class="close-btn">Ã—</span>
    <iframe id="newsletterIframe" width="540" height="700" frameborder="0" scrolling="auto" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;" src="https://867240b1.sibforms.com/serve/MUIFABwaVmPZKMEbnhKEgDq4oXoL6zYDGGJrRtALyAYzdkCyHyP2-dmrPgikI3Rltgsrmz2GdnKL95qsoypmklN1NEyX7e7lPhxGdQ9WHINmtMiTI2OHPejYZzUdd4LPYpeYh36jgAlfSGEkh-rZnWIveDy8ueTb8M5cRGx_W0Svudv2Tqkv-jTwpDY4g5HeN9c4N-7IPDwRu1kh">
    </iframe>
  </div>
</div>
<script defer="" data-version="1.3" src="/js/p.js" event-type="article" event-tags="|python|" python="" event-tag-python="true" event-release-year="2023" event-update-year="2024" event-release-month="06" event-update-month="08"></script>
<script async="" defer="" type="text/javascript">
    if (window.localStorage) {
        const urlParams = new URLSearchParams(window.location.search);
        let referral = urlParams.get('ref');
        let referer = document.referrer;
        let entryLocation = document.location;

        if (referral === '') {
            referral = null;
        }

        if (referer === '') {
            referer = null;
        }

        if (referral !== null) {
            localStorage.setItem('referral', referral);
        }

        if (null === localStorage.getItem('referer')) {
            localStorage.setItem('referer', referer);
        }

        if(null === localStorage.getItem('entry_location')) {
            localStorage.setItem('entry_location', entryLocation)
        }
    }
</script>
<script async="" defer="" src="https://cdn.scrapfly.io/static/fp/l.js" data-key="MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEXhnwGjm29RPoxvkqvzpXJQLAF9lt7jOvswaUDMYpJEVYRTb51+owAlZAl/8A19DtyJThVaLw5QjlUILQxeV5RQ=="></script>


</body></html>